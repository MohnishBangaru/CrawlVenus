

# **Phi-Ground: Deconstructing the Training Methodology for State-of-the-Art GUI Grounding**

### **1\. Executive Summary: The Phi-Ground Approach in Context**

The development of Computer Use Agents (CUAs), reminiscent of the "Jarvis" system from "Iron Man," is rapidly advancing with the progress of multimodal reasoning models. At the core of these agents lies GUI grounding, a critical component that serves as the "mechanical control" enabling a CUA to perform real-world actions like mouse clicks and keyboard inputs. The accuracy of this function directly determines the success or failure of the entire system.1  
This report presents a comprehensive analysis of the Phi-Ground model family, a state-of-the-art solution that addresses the persistent challenges in GUI grounding. Current end-to-end grounding models often fall short, with accuracy on difficult benchmarks like ScreenSpot-pro and UI-Vision still below 65%, highlighting a significant gap before they can be commercially deployed.1 The Phi-Ground research, however, demonstrates that a methodical, data-centric approach can overcome these limitations. The model family achieves state-of-the-art (SOTA) performance across all five grounding benchmarks for models with fewer than 10 billion parameters in agent settings.1 In the more challenging end-to-end model setting, where a planner is not used, the model still achieves SOTA results on three of the benchmarks, with scores of 43.2 on ScreenSpot-pro and 27.2 on UI-Vision.1  
The success of the Phi-Ground model is attributed to several key methodological innovations that are meticulously deconstructed in this report. These include a strategic two-stage architecture that decouples high-level planning from low-level perception, a robust data preparation pipeline focused on quality and distribution rather than just raw volume, and a refined understanding of scaling laws that incorporates inference-time computational load. The findings from this empirical study not only clarify the construction of effective grounding models but also provide valuable principles applicable to a wide range of other multimodal perception tasks.1

### **2\. A Foundational Architecture: Decoupling Planning from Perception**

#### **2.1. The Grand Challenge of GUI Grounding**

GUI grounding is the process by which a CUA translates a high-level instruction into a specific, executable command, such as a mouse click at a precise screen coordinate. While Large Multimodal Models (MLLMs) can effectively handle discrete commands like typing a key, they struggle to accurately identify the continuous, precise screen coordinates required for mouse clicks.1 This is the central challenge that the Phi-Ground model was designed to solve. The inherent difficulty lies in the nature of mouse commands, where a slight deviation in coordinate prediction can lead to an irreversible and harmful misclick, such as closing an unsaved file.1

#### **2.2. The Two-Stage Approach: A Strategic Design Choice**

To address the complexities of this task, the Phi-Ground methodology adopts a two-phase architecture. This approach separates the grounding task into two distinct components: spatial planning and localization.1 In the first stage, an advanced MLLM, such as GPT-4o, serves as a planner to analyze the user's task and the current screen state. It then generates a detailed, specific "Reference Expression" (RE) describing the target location. This RE is a textual description that captures the functional, positional, and visual appearance of the element to be acted upon. In the second stage, a smaller, highly specialized Phi-Ground model takes the detailed RE and the screen image as input and outputs the precise coordinates for the required action.1  
This decoupling of planning from perception is a key architectural decision. Instead of attempting to build a single, monolithic end-to-end model that must master both complex spatial reasoning and precise coordinate prediction, the Phi-Ground team offloads the former to a more capable, likely more expensive, but also more general-purpose model. This allows the Phi-Ground model to focus exclusively on the localization component, leading to superior performance on the grounding task itself. This system design is particularly well-suited for practical, deployable systems, offering a more robust and effective solution compared to traditional end-to-end models.1 The research defines three types of REs: "Short/instruction" (concise, used in end-to-end evaluation), "Long/agent" (detailed, generated by a planner for the agent setting), and "Long-gold" (highly descriptive, used exclusively for generating high-quality training data).1

#### **2.3. Findings on Input and Output Formats**

A detailed empirical study on model input and output formats revealed several important findings.1  
**Modality Order**: The order in which text (the RE) and images are input into the model has a significant impact on performance. The research found that inputting the text before the image consistently yields better results than the reverse order.1 The mechanism behind this is attributed to the causal mask in the transformer decoder's attention mechanism. When the image is processed after the textual instruction, the subsequent image tokens are able to adapt their feature modeling to be "instruction-aware," directly influenced by the preceding text. This simple modification effectively enables a form of instruction-aware visual modeling without complex architectural changes.1  
**Output Format Analysis**: The study also explored various output formats for coordinates, including:

* Point+OmniParser: Outputting a central point and using an external parser (OmniParser-V2) to select a corresponding bounding box.  
* XYXY: Outputting the top-left and bottom-right coordinates of a bounding box.  
* XYWH: Outputting the top-left coordinates along with the width and height of the bounding box.  
* MidWH: Outputting the center point along with the width and height of the bounding box.

The analysis, as detailed in Table 3, demonstrates a clear trade-off between click accuracy and bounding box precision.1

| Output format | ScreenSpot-V2 Click ACC | ScreenSpot-pro Click ACC | ScreenSpot-V2 IoU@0.8 | ScreenSpot-pro IoU@0.8 |
| :---- | :---- | :---- | :---- | :---- |
| point+OP | 85.5 | 30.6 | 22.0 (30.9) | 18.8 |
| XYXY | 84.7 | 23.8 | 72.4 (71.2) | 20.1 |
| XYWH | 84.2 | 21.8 | 58.9 (57.7) | 18.6 |
| MidWH | 85.0 | 22.4 | 57.1 (58.1) | 19.2 |

The findings indicate that directly outputting the point format with OmniParser (Point+OP) achieves the best click accuracy, while the XYXY format provides the highest precision for the bounding box. This highlights that the optimal output format depends on the specific requirements of the application, such as whether a precise click or a well-defined box is the primary objective.1

### **3\. The Anatomy of Data: Curation, Scaling, and Augmentation**

The success of the Phi-Ground model is fundamentally rooted in a massive, high-quality, and diverse data preparation pipeline. This report collected over 40 million data samples from a variety of sources, shifting the complexity from the model's loss function to the data pipeline itself.1

#### **3.1. The Multi-Source Data Preparation Pipeline**

The research team meticulously prepared data from four distinct sources to ensure comprehensive coverage and robust generalization capabilities.1

* **Open-Source Data**: This formed a baseline, utilizing approximately 10 million samples from established datasets such as OS-Atlas, SeeClick, E2ISynth, and GUIAct. All of this data was re-annotated using GPT-4o with the "Long-gold" RE generation prompt to ensure consistency and high quality.1  
* **Common Crawl Data**: To achieve a larger scale, web pages from CommonCrawl were rendered into screenshots. This process was accompanied by a highly specific data cleaning pipeline to remove noise that could cause training failures.1  
* **Web Search Data**: A high-resolution corpus was constructed using the Bing Image Search API. This dataset, which includes 158,000 samples from 66 desktop applications, was filtered with a CLIP-based classifier to retain only screenshots. The elements within these images were then annotated for bounding boxes and labeled with REs.1  
* **Human-Labeled Data**: An 80,000-sample dataset was created for in-domain training, focusing on common Windows applications. The data was collected through custom screen recording software, and bounding boxes were refined by human labelers before being annotated with REs by GPT-4o using the "Long-gold" method.1

#### **3.2. Common Crawl Data: Cleaning for Quality**

The Common Crawl pipeline was designed with meticulous cleaning and filtering steps to ensure data quality. This process, illustrated in Figure 5 of the original paper, includes index and domain deduplication to prevent model overfitting to specific website layouts.1 The pipeline also includes rule-based filtering to remove noisy data.1 For example,  
div containers that encompass multiple buttons are removed in favor of the inner, more specific elements. The team also developed a method to detect and remove empty or solid-colored boxes and filter out non-interactive text elements, which are often mistakenly classified as interactive elements due to their aspect ratio.1 This detailed approach to data curation was crucial to prevent training failures and ensure the integrity of the dataset.

#### **3.3. The Importance of Data Distribution**

An analysis of the distribution of element center points across different datasets revealed a critical spatial bias in web-rendering data. Datasets like SeeClick and Fineweb almost entirely lacked buttons on the right side of the screen, a consequence of standard web design practices that reserve the right side for scroll bars. This spatial skew is not representative of the broader GUI landscape, particularly in desktop applications.1 To counter this, the team developed a re-sampling algorithm that divides the screen into a  
50×50 grid and samples a fixed number of points from each cell. This process ensures that the central points of the selected elements are uniformly distributed across the screen.1  
This approach addresses a fundamental limitation of training on a single, biased data source. A model trained exclusively on spatially skewed web data would perform poorly on desktop environments with different layouts, where elements can appear anywhere. The re-sampling algorithm ensures that the model is exposed to a more representative distribution of potential click locations, which is vital for generalization in real-world scenarios.1

#### **3.4. The Impact of Data Augmentation**

The research revisited traditional computer vision techniques, finding that certain data augmentations can be highly effective for large models, particularly in high-resolution scenarios like ScreenSpot-pro.1 The team explored two primary techniques: Random Crop and Random Resize.1

* **Random Crop**: This technique simulates incomplete pages or elements that are partially off-screen.  
* **Random Resize**: This method shrinks the original image and places it on a fixed-size white canvas. This is particularly effective for simulating scenarios where elements appear very small, which is common in high-resolution interfaces.

The ablation study results, as shown in Table 4 of the original report, demonstrate that Random Resize significantly improves performance on the high-resolution ScreenSpot-pro benchmark, with an improvement of up to \+8.0 in click accuracy.1 This is because it forces the model to learn to perceive "micro-elements" that occupy a tiny percentage of the screen, a key challenge in professional GUI grounding.1 In contrast,  
Random Crop was found to have a minimal impact across various scenarios, a finding that challenges conventional wisdom from traditional object detection training.1

### **4\. Training Recipes and Optimization Strategies**

#### **4.1. From Trivial to Transcendent: The Case Against Complex Loss Functions**

The researchers experimented with a number of training techniques that appear theoretically sound but were found to be trivial when applied to large-scale training.1 These discarded approaches included:

* **Tokenized Coordinates**: Representing coordinates as special tokens, which was found to cause model collapse due to the introduction of a large number of unlearned tokens.1  
* **Label Smoothing**: Applying smoothing to digit tokens to approximate regression loss. This technique only showed marginal improvements with small datasets and batch sizes but lost its advantage with large-scale training.1  
* **Loss Re-weighting**: Assigning different weights to the losses of digit tokens (e.g., higher weights to hundreds place digits). This approach was unstable and also lost its benefits as training volume and batch size increased.1

The central conclusion from these experiments is that with a sufficiently large, clean, and diverse dataset, and an adequately large batch size, the model can implicitly learn complex relationships without the need for manual, hand-crafted interventions in the loss function.1 The straightforward next-token prediction (NTP) loss proved to be the most robust and scalable approach.1

#### **4.2. In-Domain Post-Training: Balancing Generality and Specialization**

In practical applications, a developer may want to fine-tune a model on a small set of target software. The report explores three strategies for this in-domain fine-tuning.1

* **Strategy A**: Directly incorporating domain-specific data into a pre-trained model.  
* **Strategy B**: Performing Supervised Fine-Tuning (SFT) of a pre-trained model using only the domain-specific data.  
* **Strategy C**: Introducing a small proportion of domain data during the initial pre-training phase, followed by SFT with a larger proportion of domain data.

The results, as shown in Table 6, reveal that Strategy C is the most effective approach. It successfully balances the model's general capabilities (maintaining high scores on general benchmarks) with its in-domain performance (achieving the best results on the Photoshop-specific benchmark).1 In contrast, Strategy B showed a significant decline in general capabilities, a phenomenon known as catastrophic forgetting.1

#### **4.3. The Unforeseen Role of Reinforcement Learning**

The report explores the use of various Reinforcement Learning (RL) algorithms for post-training, including Proximal Policy Optimization (PPO), REINFORCE, and Direct Preference Optimization (DPO). It was found that traditional RL algorithms like PPO, which rely on reasoning and exploration, did not provide a positive increase in performance on this purely perceptual task. This is because perceptual tasks lack the textual exploration and diverse rollouts that these algorithms require.1  
However, the team made a significant discovery: DPO consistently outperformed SFT, even when starting from a highly optimized, saturated pre-trained checkpoint.1 The benefits of DPO in this context do not stem from reasoning but from an ability to adaptively select data distributions and increase robustness to noise and erroneous ground truth.1 DPO achieves this by learning from both correct and incorrect model outputs, acting as a sophisticated form of curriculum learning that gradually and stably guides the model to correct its mistakes. This finding redefines the role of RL in multimodal perception, shifting its paradigm from reasoning-based improvements to robustness-based enhancements.1

### **5\. Scaling Laws and Efficiency Trade-offs**

#### **5.1. Redefining Scaling Laws for Perception**

The research introduces a novel approach to scaling laws that moves beyond the traditional focus on parameter count (N). For visual perception tasks, the computational cost during inference is also heavily influenced by the number of image tokens (D), which is determined by the image resolution and cropping strategy.1 The report proposes using the metric  
ND (parameter count multiplied by the number of image tokens) as a more comprehensive measure of computational load and a better predictor of performance and efficiency.1 This redefinition demonstrates that the Phi-Ground models are not just powerful but also support the Pareto frontier, indicating their efficiency.1

#### **5.2. Analysis of the Phi-Ground Model Family**

The team trained multiple models under a fixed computational budget of approximately 450 NVIDIA A100 GPU days.1 The results of this study, as illustrated in the paper's figures, demonstrate a clear relationship between model performance and the  
ND metric.1 For advanced benchmarks like ScreenSpot-pro and UI-Vision, the number of image tokens is a significant performance bottleneck. When the number of tokens is too low, the model cannot perceive small objects, leading to poor scores. However, the impact of increasing image tokens gradually diminishes after a certain threshold (approximately 2000 tokens), indicating a point of diminishing returns.1  
This provides a clear guideline for developers: for high-resolution tasks, investing in a higher image token count is more impactful than simply increasing the number of parameters. This efficient resource allocation strategy allows for the development of powerful models without a commensurate increase in computational cost, offering a rational and efficient training recipe for the field.1

### **6\. A Multi-Benchmark Evaluation: Assessing Generalization and Performance**

#### **6.1. Benchmarks and Evaluation Protocol**

To ensure the model's generalization capabilities and prevent systematic overfitting to a single test set, the research evaluated the Phi-Ground model family on a comprehensive suite of five benchmarks: ScreenSpot V1 & V2, ScreenSpot-Pro, UI-Vision, Showdown-click-dev, and a proprietary Gold dataset.1 This multi-benchmark approach is a direct response to the tendency of previous work to over-optimize for one or two datasets.1 The primary evaluation metric is  
**click accuracy**, which is determined by whether the model's predicted click coordinate falls within the ground truth bounding box.1  
The evaluation was conducted in two distinct settings 1:

* **End-to-end model setting**: The model is given a short, high-level instruction (short RE) and must perform the grounding task.  
* **Agent setting**: A powerful planner (e.g., GPT-4o) generates a detailed, specific instruction (long RE), which is then fed to the grounding model.

#### **6.2. Comparative Performance Analysis**

The Phi-Ground models consistently achieved SOTA results across all benchmarks in the agent setting.1 This confirms the effectiveness of the two-stage, planner-assisted architecture. On the challenging ScreenSpot-pro benchmark, the Phi-Ground model achieved an accuracy of 55.0 in the agent setting, and on UI-Vision, it scored 36.2.1 The model also surpassed the performance of commercial models like OpenAI Operator and Claude Computer Use on the Showdown benchmark.1 In the end-to-end setting, the Phi-Ground model still achieved the best results on ScreenSpot-pro, UI-Vision, and the Gold dataset, demonstrating its strong generalization capabilities even without a planner.1 The performance on ScreenSpot-V2 was relatively average, a potential trade-off of focusing on a diverse set of benchmarks rather than optimizing for a single, long-standing one.1

### **7\. Deep Dive into Failure Modes: A Cascading Error Analysis**

To understand the remaining challenges, a meticulous cascading error analysis was conducted on the ScreenSpot-Pro benchmark.1 This approach processed the 1,534 test samples in stages, from end-to-end models to planner-assisted systems with human-corrected REs.1  
The analysis revealed several distinct types of errors:

* **Planning Omissions**: Errors that arise when the end-to-end grounding model lacks the common sense or spatial reasoning to interpret a short instruction. For example, in an instruction to "Keyin keywords for the spreadsheet," the model may incorrectly click on the word "keywords" itself rather than the intended input box. A planner effectively resolves these errors by providing a detailed RE that directs the model to the "white rectangular input box".1 This type of error accounted for 19% of the total samples.  
* **Planning Errors**: These occur when the planner itself hallucinates or makes a mistake, particularly in scenarios requiring specialized application knowledge.1 For instance, if a button is in an unusual position, the planner might generate an incorrect RE based on a common assumption, leading to an erroneous grounding output. These errors accounted for 15.4% of the total samples.1  
* **Language Not Covered**: A significant portion (7.6%) of the remaining errors were attributed to the presence of languages not covered by the model's training data, such as Chinese.1  
* **Remaining Hard Errors**: The final 14.2% of errors were categorized into issues related to extreme screen aspect ratios, complex spatial tasks (e.g., "click on a blank cell in the 13th row and 8th column"), or regions that are simply difficult to describe using natural language.1

This systematic error analysis confirms the rationale behind the two-stage architecture and highlights the limitations of both the planner and the grounding model, providing a clear roadmap for future research.1

### **8\. Conclusions, Practical Implications, and Open Questions**

In conclusion, the Phi-Ground model family represents a significant step forward in GUI grounding by demonstrating that a comprehensive, data-centric methodology can overcome the limitations of traditional approaches. The research successfully developed a rational and efficient training recipe that led to state-of-the-art performance across diverse benchmarks.1 The core findings—that separating planning from perception is a powerful architectural strategy, that large-scale data curation and quality are more impactful than complex loss functions, and that RL can enhance perceptual robustness—provide valuable guidance for the field.1  
The practical implications of this research are substantial. Developers can now leverage the insights on data distribution and augmentation to improve model generalization. The analysis of scaling laws provides clear guidance on the optimal trade-off between model size and image token count for a given computational budget.1 The success of the two-stage approach offers a blueprint for building reliable and scalable CUAs.  
However, the report also raises critical societal and ethical questions. The potential for irreversible and harmful actions by CUAs highlights the need for a robust system of accountability and human-computer collaboration.1 The risk of user privacy breaches when screen data is uploaded to the cloud necessitates the development of new legal frameworks and security protocols.1 Finally, the research concludes that a new benchmark is needed to evaluate the potential harmfulness of grounding errors, which often lead to unintended but still interactive outputs.1 This work not only advances GUI grounding but also contributes to a broader discussion on the responsible and reliable deployment of multimodal agents.

#### **Works cited**

1. Phi-Ground Tech Report.pdf  
2. Phi-Ground Tech Report: Advancing Perception in GUI Grounding \- Hugging Face, accessed August 26, 2025, [https://huggingface.co/papers/2507.23779](https://huggingface.co/papers/2507.23779)  
3. Phi-Ground Tech Report: Advancing Perception in GUI Grounding \- arXiv, accessed August 26, 2025, [https://arxiv.org/html/2507.23779](https://arxiv.org/html/2507.23779)  
4. Phi-Ground Tech Report \- GitHub Pages, accessed August 26, 2025, [https://zhangmiaosen2000.github.io/Phi-Ground/](https://zhangmiaosen2000.github.io/Phi-Ground/)