{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b8b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prompt construction utilities for Explorer (the GPT-driven Android tester).\n",
    "\n",
    "Phase-3 Step-1: Build a compact, structured prompt where the LLM acts as a\n",
    "Human App Critic, analysing UI/UX, standout features, and generic bits.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5d6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f611454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb72914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef973a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ..vision.models import UIElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = (\n",
    "    \"You are a seasoned Human App Critic. You analyse mobile apps from both \"\n",
    "    \"a user-experience and functionality standpoint. At each screen you will \"\n",
    "    \"receive: 1) a list of visible UI elements with bounding-box positions, \"\n",
    "    \"2) the task the automation is trying to complete, and 3) a short action \"\n",
    "    \"history. Your goals:\\n\"\n",
    "    \"  • Identify notable UI / UX details that stand out (good or bad).\\n\"\n",
    "    \"  • Note the BEST features offered on this screen.\\n\"\n",
    "    \"  • Point out features that feel GENERIC or uninteresting.\\n\"\n",
    "    \"  • Decide the next UI action that moves closer to completing the task.\\n\"\n",
    "    \"Return a JSON object with keys:\\n\"\n",
    "    \"    critic_notes: string   # your analysis in <120 words\\n\"\n",
    "    \"    next_action:          # JSON describing the UI action\\n\"\n",
    "    \"        type: tap|swipe|input_text|wait|key_event\\n\"\n",
    "    \"        ... parameters depending on type\\n\"\n",
    "    \"    confidence: float     # 0-1 how sure you are about next_action\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace7a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _serialize_elements(elements: list[UIElement], max_elems: int = 20) -> str:\n",
    "    \"\"\"Convert UIElement list to a concise multiline string.\"\"\"\n",
    "    lines: list[str] = []\n",
    "    for idx, el in enumerate(elements[:max_elems], 1):\n",
    "        x1, y1, x2, y2 = el.bbox.as_tuple()\n",
    "        bbox = f\"({x1},{y1})-({x2},{y2})\"\n",
    "        line = f\"{idx}. [{el.element_type}] {el.text!r} {bbox} conf={el.confidence:.2f}\"\n",
    "        lines.append(line)\n",
    "    if len(elements) > max_elems:\n",
    "        lines.append(f\"… and {len(elements) - max_elems} more elements omitted\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(\n",
    "    state: dict[str, Any],\n",
    "    task: str,\n",
    "    history: list[dict[str, Any]] | None = None,\n",
    ") -> str:\n",
    "    \"\"\"Return full prompt string for LLM call.\"\"\"\n",
    "    elements: list[UIElement] = state.get(\"ui_elements\", [])  # type: ignore[assignment]\n",
    "    elements_str = _serialize_elements(elements)\n",
    "\n",
    "    hist_lines: list[str] = []\n",
    "    if history:\n",
    "        for h in history[-5:]:  # last 5 actions\n",
    "            hist_lines.append(f\"• {h['action']['type']} => {h['result']['success']}\")\n",
    "    history_str = \"\\n\".join(hist_lines) if hist_lines else \"(no previous actions)\"\n",
    "\n",
    "    prompt = (\n",
    "        f\"{HEADER}\\n\"\n",
    "        f\"=== Current Task ===\\n{task}\\n\"\n",
    "        f\"=== Visible Elements ===\\n{elements_str}\\n\"\n",
    "        f\"=== Recent Actions ===\\n{history_str}\\n\"\n",
    "        f\"### JSON RESPONSE ONLY ###\"\n",
    "    )\n",
    "\n",
    "    logger.debug(\"Prompt generated, {0} characters\", len(prompt))\n",
    "    return prompt "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
